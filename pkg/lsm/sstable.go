package lsm

import (
	"bytes"
	"encoding/binary"
	"errors"
	"fmt"
	"os"
	"sort"

	bloom "github.com/bits-and-blooms/bloom/v3"
)

// --- On-disk basics ---

const (
	// sstMagic is a placeholder magic number for SSTable files.
	sstMagic   uint64 = 0x626c6b537354626c
	sstVersion uint32 = 1
)

// BlockHandle represents a [offset, length] region in the SSTable file.
type BlockHandle struct {
	Offset uint64
	Length uint64
}

// Footer is placed at the end of the file and references the index and filter blocks.
type Footer struct {
	IndexHandle  BlockHandle
	FilterHandle BlockHandle
	Version      uint32
	Magic        uint64
}

// --- Compression interfaces (stubs) ---

// Compressor defines an interface for optional block compression.
type Compressor interface {
	Name() string
	Compress(in []byte) ([]byte, error)
	Decompress(in []byte) ([]byte, error)
}

// noCompression is a stub that performs no compression.
type noCompression struct{}

func (n *noCompression) Name() string { return "none" }

func (n *noCompression) Compress(in []byte) ([]byte, error) { return in, nil }

func (n *noCompression) Decompress(in []byte) ([]byte, error) { return in, nil }

// --- Filter policy interfaces (Bloom stub) ---

// FilterPolicy provides a per-table filter (e.g., Bloom) to avoid unnecessary IO.
type FilterPolicy interface {
	Name() string
	MayContain(key []byte) bool
	Add(key []byte)
	WriteToBuffer() ([]byte, error)
	ReadFromBuffer([]byte) error
}

// BloomPolicy is a stub implementation with configurable false positive rate.
type BloomPolicy struct {
	FpRate float64
	Filter *bloom.BloomFilter
}

func (b *BloomPolicy) Name() string { return "bloom" }

func (b *BloomPolicy) MayContain(userKey []byte) bool {
	return b.Filter.Test(userKey)
}

func (b *BloomPolicy) Add(key []byte) {
	b.Filter.Add(key)
}

func (b *BloomPolicy) WriteToBuffer() ([]byte, error) {
	var buf bytes.Buffer
	_, err := b.Filter.WriteTo(&buf)
	if err != nil {
		return nil, err
	}
	return buf.Bytes(), nil
}

func (b *BloomPolicy) ReadFromBuffer(buf []byte) error {
	_, err := b.Filter.ReadFrom(bytes.NewReader(buf))
	return err
}

// --- Block builders/readers (skeleton) ---

// blockBuilder accumulates KV pairs into a data block using prefix/delta encoding
/*
Cursor generated blockBuilder code
Implemented full LevelDB-style blockBuilder:
Add now encodes entries as varints [shared][unshared][valLen] then appends unshared key and value.
Starts a restart at every 16th entry and records its block offset.
Finish appends restart offsets and the count (little-endian uint32).
Added commonPrefixLen helper.
This seals blocks with restart metadata and prefix-compressed keys, ready for compression and writing.
*/
type blockBuilder struct {
	buf      []byte
	restarts []uint32
	counter  int
	lastKey  []byte
}

func (b *blockBuilder) Reset() {
	b.buf = b.buf[:0]
	b.restarts = b.restarts[:0]
	b.counter = 0
	b.lastKey = nil
}

// Add appends an entry encoded as [shared][unshared][valLen][unsharedKey][value].
// shared/unshared/valLen are varints. Every 16 entries we start a new restart (shared=0).
// Generated by Cursor
func (b *blockBuilder) Add(key []byte, value []byte) {
	const restartInterval = 16
	var shared int
	if b.counter%restartInterval == 0 {
		// Start of a restart point: remember block offset; no prefix sharing
		b.restarts = append(b.restarts, uint32(len(b.buf)))
		shared = 0
	} else {
		shared = commonPrefixLen(b.lastKey, key)
	}
	unshared := len(key) - shared

	var hdr [10]byte
	n := binary.PutUvarint(hdr[:], uint64(shared))
	b.buf = append(b.buf, hdr[:n]...)
	n = binary.PutUvarint(hdr[:], uint64(unshared))
	b.buf = append(b.buf, hdr[:n]...)
	n = binary.PutUvarint(hdr[:], uint64(len(value)))
	b.buf = append(b.buf, hdr[:n]...)
	b.buf = append(b.buf, key[shared:]...)
	b.buf = append(b.buf, value...)

	b.counter++
	b.lastKey = append(b.lastKey[:0], key...)
}

// Generated by Cursor
func (b *blockBuilder) Finish() []byte {
	// Ensure at least one restart (offset 0) so readers have a baseline.
	if len(b.restarts) == 0 {
		b.restarts = append(b.restarts, 0)
	}
	// Append restart offsets (uint32 little-endian) followed by restart count.
	for _, r := range b.restarts {
		var tmp [4]byte
		binary.LittleEndian.PutUint32(tmp[:], r)
		b.buf = append(b.buf, tmp[:]...)
	}
	var cnt [4]byte
	binary.LittleEndian.PutUint32(cnt[:], uint32(len(b.restarts)))
	b.buf = append(b.buf, cnt[:]...)
	return b.buf
}

// commonPrefixLen returns the length of the common prefix of a and b.
func commonPrefixLen(a, b []byte) int {
	n := len(a)
	if len(b) < n {
		n = len(b)
	}
	i := 0
	for i < n && a[i] == b[i] {
		i++
	}
	return i
}

type tableIter struct {
	tr       *tableReader
	seqLimit uint64
	blockIdx int
	curIter  *blockIter
	valid    bool
}

func (it *tableIter) First() {
	it.blockIdx = 0
	it.loadBlockAndFirst()
}

// find block: last sepKey <= userKey using table index
// then seek userKey within the block
func (it *tableIter) Seek(userKey []byte) {
	i := sort.Search(len(it.tr.indexEntries), func(i int) bool {
		return bytes.Compare(it.tr.indexEntries[i].SepKey, userKey) > 0
	})
	// No such block ==> return
	if i == 0 {
		it.valid = false
		return
	}
	// Locate to the correct position
	it.blockIdx = i - 1
	if !it.loadBlock() {
		it.valid = false
		return
	}
	it.curIter.Seek(userKey)
	if !it.curIter.Valid() {
		it.advanceBlock()
	}
	it.valid = it.curIter != nil && it.curIter.Valid()
}

func (it *tableIter) Next() {
	if !it.valid {
		return
	}
	it.curIter.Next()
	if !it.curIter.Valid() {
		it.advanceBlock()
	}
	it.valid = it.curIter != nil && it.curIter.Valid()
}

func (it *tableIter) Valid() bool {
	return it.curIter != nil && it.curIter.Valid()
}

func (it *tableIter) Key() []byte {
	if !it.valid {
		return nil
	}
	return it.curIter.Key()
}

func (it *tableIter) Value() []byte {
	if !it.valid {
		return nil
	}
	return it.curIter.Value()
}

func (it *tableIter) Close() error {
	if it.curIter != nil {
		it.curIter.Close()
	}
	it.curIter = nil
	it.valid = false
	return nil
}

// helpers
// Loads the current block into the iterator and returns true if successful.
func (it *tableIter) loadBlock() bool {
	if it.blockIdx < 0 || it.blockIdx >= len(it.tr.indexEntries) {
		return false
	}
	h := it.tr.indexEntries[it.blockIdx].Hdl
	enc := make([]byte, int(h.Length))
	if _, err := it.tr.f.ReadAt(enc, int64(h.Offset)); err != nil {
		return false
	}
	raw, err := it.tr.compressor.Decompress(enc)
	if err != nil {
		return false
	}
	br, ok := newBlockReader(raw)
	if !ok {
		return false
	}
	it.curIter = newBlockIter(br)
	return true
}

func (it *tableIter) loadBlockAndFirst() {
	if !it.loadBlock() {
		it.valid = false
		return
	}
	it.curIter.First()
	it.valid = it.curIter != nil && it.curIter.Valid()
}

// advances block should skip empty blocks
// Empty blocks E.g.: Snapshot visibility/tombstones (when implemented): all entries in the block are invisible at seqLimit.
func (it *tableIter) advanceBlock() {
	for {
		it.blockIdx++
		if !it.loadBlock() {
			it.curIter = nil
			return
		}
		// position block iterator at the first entry of the new block
		it.curIter.First()
		if it.curIter.Valid() {
			return
		}
	}
}

/* Block Level Operations are generated by Cursor */

// blockReader reads a single data block built by blockBuilder.
// It understands the [shared][unshared][valLen][unsharedKey][value] encoding and restart array.
type blockReader struct {
	data          []byte   // entries region (excludes restart array)
	restarts      []uint32 // offsets into data
	restartsStart int      // index in the original block where restart array begins
	numRestarts   int
}

// newBlockReader parses a block into a readable structure. Returns false if malformed.
func newBlockReader(block []byte) (blockReader, bool) {
	if len(block) < 4 {
		return blockReader{}, false
	}
	num := int(binary.LittleEndian.Uint32(block[len(block)-4:]))
	if num <= 0 {
		return blockReader{}, false
	}
	rsStart := len(block) - 4*(num+1)
	if rsStart < 0 {
		return blockReader{}, false
	}
	restarts := make([]uint32, num)
	for i := 0; i < num; i++ {
		off := rsStart + i*4
		restarts[i] = binary.LittleEndian.Uint32(block[off : off+4])
		// basic sanity: restart offsets must point inside data region
		if int(restarts[i]) < 0 || int(restarts[i]) >= rsStart {
			return blockReader{}, false
		}
	}
	br := blockReader{
		data:          block[:rsStart],
		restarts:      restarts,
		restartsStart: rsStart,
		numRestarts:   num,
	}
	// touch fields to avoid unused warnings in skeleton stage
	_ = br.numRestarts
	return br, true
}

// Get performs an in-block lookup by binary searching restart points, then scanning forward.
func (br blockReader) Get(target []byte) (val []byte, ok bool) {
	if br.numRestarts == 0 {
		return nil, false
	}
	// Binary search restart array for the last restart whose first key <= target
	low, high := 0, br.numRestarts-1
	for low < high {
		mid := (low + high + 1) / 2 // bias to the right
		k, _, _, ok := br.peekFirstKeyAtRestart(mid)
		if !ok {
			return nil, false
		}
		if bytes.Compare(k, target) <= 0 {
			low = mid
		} else {
			high = mid - 1
		}
	}
	// Linear scan from chosen restart
	offset := int(br.restarts[low])
	var prevKey []byte
	for offset < br.restartsStart {
		key, value, next, ok := br.decodeEntryAt(offset, prevKey)
		if !ok {
			return nil, false
		}
		cmp := bytes.Compare(key, target)
		if cmp == 0 {
			return value, true
		}
		if cmp > 0 {
			return nil, false
		}
		prevKey = key
		offset = next
	}
	return nil, false
}

// blockIter iterates entries within a single block built by blockBuilder.
type blockIter struct {
	br         blockReader
	restartIdx int
	offset     int
	prevKey    []byte
	key        []byte
	val        []byte
	next       int
	valid      bool
}

func newBlockIter(br blockReader) *blockIter {
	it := &blockIter{br: br}
	it.First()
	return it
}

// First positions the iterator at the first entry in the block.
func (it *blockIter) First() {
	if it.br.numRestarts == 0 {
		it.valid = false
		return
	}
	it.restartIdx = 0
	it.offset = int(it.br.restarts[0])
	it.prevKey = nil
	it.decodeCurrent()
}

// Seek positions at the first key >= target within this block.
func (it *blockIter) Seek(target []byte) {
	if it.br.numRestarts == 0 {
		it.valid = false
		return
	}
	low, high := 0, it.br.numRestarts-1
	for low < high {
		mid := (low + high + 1) / 2
		k, _, _, ok := it.br.peekFirstKeyAtRestart(mid)
		if !ok {
			it.valid = false
			return
		}
		if bytes.Compare(k, target) <= 0 {
			low = mid
		} else {
			high = mid - 1
		}
	}
	it.restartIdx = low
	it.offset = int(it.br.restarts[low])
	it.prevKey = nil
	// Linear scan until >= target
	it.decodeCurrent()
	for it.valid && bytes.Compare(it.key, target) < 0 {
		it.Next()
	}
}

// Next advances to the next entry.
func (it *blockIter) Next() {
	if !it.valid {
		return
	}
	it.prevKey = append(it.prevKey[:0], it.key...)
	it.offset = it.next
	if it.offset >= it.br.restartsStart {
		it.valid = false
		return
	}
	it.decodeCurrent()
}

func (it *blockIter) Valid() bool { return it.valid }

func (it *blockIter) Key() []byte {
	if !it.valid {
		return nil
	}
	return it.key
}

func (it *blockIter) Value() []byte {
	if !it.valid {
		return nil
	}
	return it.val
}

func (it *blockIter) Close() error { return nil }

func (it *blockIter) decodeCurrent() {
	key, val, next, ok := it.br.decodeEntryAt(it.offset, it.prevKey)
	if !ok {
		it.valid = false
		return
	}
	it.key = key
	it.val = val
	it.next = next
	it.valid = true
}

// --- blockReader helpers ---

// peekFirstKeyAtRestart decodes the first key at the given restart index.
func (br blockReader) peekFirstKeyAtRestart(idx int) (key []byte, value []byte, next int, ok bool) {
	if idx < 0 || idx >= br.numRestarts {
		return nil, nil, 0, false
	}
	offset := int(br.restarts[idx])
	// At restart points, shared must be 0, but we still parse generically
	return br.decodeEntryAt(offset, nil)
}

// decodeEntryAt decodes one entry starting at offset using prevKey for prefix reconstruction.
// Returns full key, value slice (referencing underlying block), next offset, and ok flag.
func (br blockReader) decodeEntryAt(offset int, prevKey []byte) ([]byte, []byte, int, bool) {
	if offset < 0 || offset >= br.restartsStart {
		return nil, nil, 0, false
	}
	// read varints: shared, unshared, valLen
	shared, n1 := binary.Uvarint(br.data[offset:])
	if n1 <= 0 {
		return nil, nil, 0, false
	}
	offset += n1
	unshared, n2 := binary.Uvarint(br.data[offset:])
	if n2 <= 0 {
		return nil, nil, 0, false
	}
	offset += n2
	valLen, n3 := binary.Uvarint(br.data[offset:])
	if n3 <= 0 {
		return nil, nil, 0, false
	}
	offset += n3

	szShared := int(shared)
	szUnshared := int(unshared)
	szVal := int(valLen)
	if szShared > len(prevKey) {
		return nil, nil, 0, false
	}
	// bounds: need szUnshared + szVal bytes available in data region
	if offset+szUnshared+szVal > br.restartsStart {
		return nil, nil, 0, false
	}
	fullKey := make([]byte, szShared+szUnshared)
	copy(fullKey, prevKey[:szShared])
	copy(fullKey[szShared:], br.data[offset:offset+szUnshared])
	offset += szUnshared
	value := br.data[offset : offset+szVal]
	next := offset + szVal
	return fullKey, value, next, true
}

// keep references to avoid 'unused' warnings during the skeleton stage
var (
	_ = blockReader{}
	_ = blockIter{}
)

// --- Index and filter builders (skeleton) ---

type indexEntry struct {
	SepKey []byte
	Hdl    BlockHandle
}

type indexBuilder struct{ entries []indexEntry }

func (ib *indexBuilder) Add(sepKey []byte, h BlockHandle) {
	ib.entries = append(ib.entries, indexEntry{SepKey: append([]byte(nil), sepKey...), Hdl: h})
}

func (ib *indexBuilder) Finish() []byte {
	// trivial encoding: [kLen][k][offset(8)][length(8)] per entry
	var buf []byte
	var hdr [10]byte
	for _, e := range ib.entries {
		n := binary.PutUvarint(hdr[:], uint64(len(e.SepKey)))
		buf = append(buf, hdr[:n]...)
		buf = append(buf, e.SepKey...)
		tmp := make([]byte, 8)
		binary.LittleEndian.PutUint64(tmp, e.Hdl.Offset)
		buf = append(buf, tmp...)
		binary.LittleEndian.PutUint64(tmp, e.Hdl.Length)
		buf = append(buf, tmp...)
	}
	return buf
}

type filterBuilder struct {
	policy FilterPolicy
	// Accumulating user keys for filter construction
	keys    [][]byte
	lastKey []byte
}

func (fb *filterBuilder) AddKey(userKey []byte) {
	// skip duplicates when keys are grouped by userKey
	if len(fb.lastKey) > 0 && bytes.Equal(fb.lastKey, userKey) {
		return
	}
	fb.lastKey = append(fb.lastKey[:0], userKey...)
	fb.keys = append(fb.keys, append([]byte(nil), userKey...))
}

func (fb *filterBuilder) Finish() []byte {
	if fb.policy == nil {
		return nil
	}
	for i := range fb.keys {
		fb.policy.Add(fb.keys[i])
	}

	data, _ := fb.policy.WriteToBuffer()
	return data
}

// --- Table writer (skeleton) ---

type tableWriter struct {
	// destination (e.g., *os.File) will be added in a later step
	f           *os.File
	opts        Options
	cmp         func(a, b []byte) int
	blockSize   int
	compressor  Compressor
	filter      *filterBuilder
	dataBuilder *blockBuilder
	index       *indexBuilder

	// rolling state
	pendingFirstKey []byte
	offset          uint64

	// ordering check, for global state
	lastUserKey []byte
	lastSeq     uint64
	seenAny     bool

	// size estimate for current block
	curBlockApprox int
}

// Add expects InternalKey ordered by userKey asc and seq desc.
// 会被db调用 按照immutable memtable的顺序写入ss table, 所以ikey是有序的
func (tw *tableWriter) Add(ikey InternalKey, value []byte) error {
	if tw.seenAny {
		c := bytes.Compare(ikey.UserKey, tw.lastUserKey)
		if c < 0 {
			return errors.New("tableWriter.Add: out-of-order userKey")
		}
		if c == 0 && ikey.Seq > tw.lastSeq {
			return errors.New("tableWriter.Add: out-of-order seq for same userKey")
		}
	} else {
		tw.seenAny = true
	}
	tw.lastUserKey = append(tw.lastUserKey[:0], ikey.UserKey...)
	tw.lastSeq = ikey.Seq

	// if it is the start of the new block
	if tw.curBlockApprox == 0 && len(tw.pendingFirstKey) == 0 {
		tw.pendingFirstKey = append(tw.pendingFirstKey[:0], ikey.UserKey...)
		// indicate start of a new data block to filter (boundary will be finalized on flush)
	}
	// Write to current block
	tw.dataBuilder.Add(ikey.UserKey, value)
	tw.curBlockApprox += len(ikey.UserKey) + len(value) + 8

	if tw.filter != nil {
		tw.filter.AddKey(ikey.UserKey)
	}

	if tw.curBlockApprox >= tw.blockSize {
		if err := tw.flushCurrentBlock(); err != nil {
			return err
		}
	}
	return nil
}

// Flush partial block, write to filter, index and write footer
// Calls Fsync for file write then done
func (tw *tableWriter) Finish() (Footer, error) {
	// flush last partial block
	if tw.curBlockApprox > 0 {
		if err := tw.flushCurrentBlock(); err != nil {
			return Footer{}, err
		}
	}

	var indexH, filterH BlockHandle
	// 2) Write index block.
	if tw.index != nil {
		idx := tw.index.Finish()
		if len(idx) > 0 {
			out, err := tw.compressor.Compress(idx)
			if err != nil {
				return Footer{}, err
			}
			if err := writeAtAll(tw.f, int64(tw.offset), out); err != nil {
				return Footer{}, err
			}
			indexH = BlockHandle{Offset: tw.offset, Length: uint64(len(out))}
			tw.offset += uint64(len(out))
		}
	}

	// 3) Write filter block.
	if tw.filter != nil {
		filter := tw.filter.Finish()
		if len(filter) > 0 {
			out, err := tw.compressor.Compress(filter)
			if err != nil {
				return Footer{}, err
			}
			if err := writeAtAll(tw.f, int64(tw.offset), out); err != nil {
				return Footer{}, err
			}
			filterH = BlockHandle{Offset: tw.offset, Length: uint64(len(out))}
			tw.offset += uint64(len(out))
		}
	}

	footer := Footer{
		IndexHandle:  indexH,
		FilterHandle: filterH,
		Version:      sstVersion,
		Magic:        sstMagic,
	}

	// 4) Write footer
	fb := encodeFooter(footer)
	if err := writeAtAll(tw.f, int64(tw.offset), fb); err != nil {
		return Footer{}, err
	}
	tw.offset += uint64(len(fb))

	// 5) Fsync
	if err := tw.f.Sync(); err != nil {
		return Footer{}, err
	}
	return footer, nil
}

// Handles file close
func (tw *tableWriter) Close() error {
	if err := tw.f.Close(); err != nil {
		return err
	}
	return nil
}

// flushCurrentBlock finalizes and writes the current data block
func (tw *tableWriter) flushCurrentBlock() error {
	raw := tw.dataBuilder.Finish()
	out, err := tw.compressor.Compress(raw)
	if err != nil {
		return err
	}

	if err := writeAtAll(tw.f, int64(tw.offset), out); err != nil {
		return err
	}
	h := BlockHandle{Offset: tw.offset, Length: uint64(len(out))}
	if tw.index != nil && len(tw.pendingFirstKey) > 0 {
		tw.index.Add(tw.pendingFirstKey, h)
	}
	tw.offset += uint64(len(out))
	tw.dataBuilder.Reset()
	// reset to empty state for the next block
	tw.pendingFirstKey = tw.pendingFirstKey[:0]
	tw.curBlockApprox = 0

	// don't need to reset these because they maintain global state for ALL blocks in SST
	// tw.seenAny = false
	// tw.lastUserKey = tw.lastUserKey[:0]
	// tw.lastSeq = 0
	return nil
}

// NewTableWriter constructs a new file-backed table writer.
func NewTableWriter(f *os.File, opts Options) (*tableWriter, error) {
	if f == nil {
		return nil, errors.New("nil file")
	}
	blkSize := opts.BlockSize
	if blkSize <= 0 {
		blkSize = 4 << 10 // 4 * 2^10 = 4KB. 2^10 is 1KB
	}
	tw := &tableWriter{
		f:           f,
		opts:        opts,
		cmp:         bytes.Compare,
		blockSize:   blkSize,
		compressor:  pickCompressor(opts.Compression), // No compression for now
		filter:      &filterBuilder{policy: defaultFilter(opts.BloomFpRate)},
		dataBuilder: &blockBuilder{},
		index:       &indexBuilder{},
	}
	return tw, nil
}

// helpers
func writeAtAll(f *os.File, offset int64, p []byte) error {
	bytesToWrite := len(p)
	for bytesToWrite > 0 {
		n, err := f.WriteAt(p, offset)
		if err != nil {
			return err
		}
		bytesToWrite -= n
		offset += int64(n)
		p = p[n:]
	}
	return nil
}

// Generated by Cursor
func encodeFooter(f Footer) []byte {
	buf := make([]byte, 0, 44)
	put64 := func(x uint64) {
		tmp := make([]byte, 8)
		binary.LittleEndian.PutUint64(tmp, x)
		buf = append(buf, tmp...)
	}
	put32 := func(x uint32) {
		tmp := make([]byte, 4)
		binary.LittleEndian.PutUint32(tmp, x)
		buf = append(buf, tmp...)
	}
	put64(f.IndexHandle.Offset)
	put64(f.IndexHandle.Length)
	put64(f.FilterHandle.Offset)
	put64(f.FilterHandle.Length)
	put32(f.Version)
	put64(f.Magic)
	return buf
}

// --- Table reader (skeleton) ---

type tableReader struct {
	f          *os.File
	opts       Options
	compressor Compressor
	filter     FilterPolicy
	indexData  []byte
	footer     Footer
	// parsed index for faster lookups
	indexEntries []indexEntry
}

func OpenTable(f *os.File, opts Options) (*tableReader, error) {
	if f == nil {
		return nil, errors.New("nil file")
	}
	footer, err := readFooter(f)
	if err != nil {
		return nil, err
	}
	tr := &tableReader{
		f:          f,
		opts:       opts,
		compressor: pickCompressor(opts.Compression),
		filter:     defaultFilter(opts.BloomFpRate),
	}

	// persist decoded footer
	tr.footer = footer

	// Read Index Block, decompress if needed
	if footer.IndexHandle.Length > 0 {
		buf := make([]byte, int(footer.IndexHandle.Length))
		if _, err := f.ReadAt(buf, int64(footer.IndexHandle.Offset)); err != nil {
			return nil, err
		}
		dec, err := tr.compressor.Decompress(buf)
		if err != nil {
			return nil, err
		}
		tr.indexData = dec
		// parse index once at open time
		entries, err := parseIndexBlock(tr.indexData)
		if err != nil {
			return nil, err
		}
		tr.indexEntries = entries
	}

	// Read filter Block
	if footer.FilterHandle.Length > 0 {
		buf := make([]byte, int(footer.FilterHandle.Length))
		if _, err := f.ReadAt(buf, int64(footer.FilterHandle.Offset)); err != nil {
			return nil, err
		}
		if err := tr.filter.ReadFromBuffer(buf); err != nil {
			return nil, err
		}
	}

	return tr, nil
}

// Single point lookup
func (tr *tableReader) Get(userKey []byte, seqLimit uint64) ([]byte, bool, error) {
	// Fast negative: if filter says not present, skip IO
	if tr.filter != nil && !tr.filter.MayContain(userKey) {
		return nil, false, nil
	}
	// Could be a false positive; proceed to index/data lookup

	if len(tr.indexEntries) == 0 {
		return nil, false, nil
	}

	// Find last sepKey > userKey ==> upperbound
	i := sort.Search(len(tr.indexEntries), func(i int) bool {
		return bytes.Compare(tr.indexEntries[i].SepKey, userKey) > 0
	})
	if i == 0 {
		return nil, false, nil
	}
	h := tr.indexEntries[i-1].Hdl

	// Read and decompress block
	enc := make([]byte, int(h.Length))
	if _, err := tr.f.ReadAt(enc, int64(h.Offset)); err != nil {
		return nil, false, err
	}
	raw, err := tr.compressor.Decompress(enc)
	if err != nil {
		return nil, false, err
	}

	br, ok := newBlockReader(raw)
	if !ok {
		return nil, false, fmt.Errorf("malformed block")
	}
	val, found := br.Get(userKey)
	if !found {
		return nil, false, nil
	}
	return val, true, nil
}

// parseIndexBlock decodes the index block bytes into entries.
// Index: [kLen][k][offset(8)][length(8)] per entry
func parseIndexBlock(data []byte) ([]indexEntry, error) {
	var out []indexEntry
	off := 0
	for off < len(data) {
		kLen, n := binary.Uvarint(data[off:])
		if n <= 0 {
			return nil, fmt.Errorf("bad varint in index")
		}
		off += n
		if off+int(kLen) > len(data) {
			return nil, fmt.Errorf("index key oob")
		}
		key := append([]byte(nil), data[off:off+int(kLen)]...)
		off += int(kLen)
		if off+16 > len(data) {
			return nil, fmt.Errorf("index handle oob")
		}
		off64 := binary.LittleEndian.Uint64(data[off : off+8])
		off += 8
		len64 := binary.LittleEndian.Uint64(data[off : off+8])
		off += 8
		out = append(out, indexEntry{SepKey: key, Hdl: BlockHandle{Offset: off64, Length: len64}})
	}
	return out, nil
}

// Range query lookup ==> generates an iterator over the entire table
// So that on the db layer, db can merge iterators from sst, memtable, db cache, etc.
func (tr *tableReader) NewIterator(ro *ReadOptions) Iterator {
	// TODO: implement
	return nil
}

func (tr *tableReader) Close() error {
	if err := tr.f.Close(); err != nil {
		return err
	}
	// reference fields to avoid unused field warnings in skeleton stage
	_ = tr.indexData
	_ = tr.footer
	return nil
}

// --- Option wiring helpers ---

func pickCompressor(_ string) Compressor { return &noCompression{} }

func defaultFilter(fpRate float64) FilterPolicy {
	return &BloomPolicy{FpRate: fpRate, Filter: bloom.NewWithEstimates(1000000, fpRate)}
}

// The last 44 bytes of the file is the footer
const footerSize = 44

func readFooter(f *os.File) (Footer, error) {
	st, err := f.Stat()
	if err != nil {
		return Footer{}, err
	}
	if st.Size() < footerSize {
		return Footer{}, fmt.Errorf("file too small: %d", st.Size())
	}
	buf := make([]byte, footerSize)
	if _, err := f.ReadAt(buf, st.Size()-footerSize); err != nil {
		return Footer{}, err
	}

	// Decode footer...
	var ft Footer
	ft.IndexHandle.Offset = binary.LittleEndian.Uint64(buf[0:8])
	ft.IndexHandle.Length = binary.LittleEndian.Uint64(buf[8:16])
	ft.FilterHandle.Offset = binary.LittleEndian.Uint64(buf[16:24])
	ft.FilterHandle.Length = binary.LittleEndian.Uint64(buf[24:32])
	ft.Version = binary.LittleEndian.Uint32(buf[32:36])
	ft.Magic = binary.LittleEndian.Uint64(buf[36:44])

	if ft.Magic != sstMagic {
		return Footer{}, fmt.Errorf("bad magic: %x", ft.Magic)
	}
	// optionally check version
	// if ft.Version != sstVersion { ... }

	return ft, nil
}
